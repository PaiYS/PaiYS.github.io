---
layout: page
title: RadarHand
description: a Wrist-Worn Radar for On-Skin Touch-based Proprioceptive Gestures
img: assets/img/radarhand_main.png
importance: 0.09
category: work
youtubeId: BxbbxNes9vI
---

{% include youtubePlayer.html id=page.youtubeId %}

We introduce RadarHand, a wrist-worn wearable with millimetre wave radar that detects on-skin touch-based proprioceptive hand gestures. Radars are robust, private, small, penetrate materials, and require low computation costs. 
We first evaluated the proprioceptive and tactile perception nature of the back of the hand and found that tapping on the thumb is the least proprioceptive error of all the finger joints, followed by the index finger, middle finger, ring finger, and pinky finger in the eyes-free and high cognitive load situation. Next, we trained deep-learning models for gesture classification. We introduce two types of gestures based on the locations of the back of the hand: generic gestures and discrete gestures. Discrete gestures are gestures that start at specific locations and end at specific locations at the back of the hand, in contrast to generic gestures, which can start anywhere and end anywhere on the back of the hand. Out of 27 gesture group possibilities, we achieved 92% accuracy for a  set of seven gestures and 93% accuracy for the set of eight discrete gestures. Finally, we evaluated RadarHand's performance in real-time under two interaction modes: Active interaction and Reactive interaction. Active
interaction is where the user initiates input to achieve the desired output, and reactive interaction is where the device initiates interaction and requires the user to react. We obtained an accuracy of 87% and 74% for active generic and discrete gestures, respectively, as well as 91% and 81.7% for reactive generic and discrete gestures, respectively. We discuss the implications of RadarHand for gesture recognition and directions for future works.

Paper link <a href='https://yunsuenpai.com/assets/pdf/radarhand.pdf'>here</a>.

